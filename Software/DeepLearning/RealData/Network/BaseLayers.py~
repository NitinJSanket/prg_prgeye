#!/usr/bin/env python

# Run as python -m SimilarityNet.Network.SqueezeNet from DeepLearning folder

import tensorflow as tf
import sys
import numpy as np
import inspect
from functools import wraps
from tensorflow.contrib.framework import add_arg_scope
from tensorflow.contrib.framework import arg_scope
# Required to import ..Misc so you don't have to run as package with -m flag
# sys.path.insert(0, '../Misc/')
# import TFUtils as tu
# from Decorators import *
from ..Misc import TFUtils as tu
from ..Misc.Decorators import *
import ..Misc.warpICSTN2 as warp2

# TODO: Add training flag
    
class BaseLayers(object):
    def __init__(self):
        self.CurrBlock = 0
    # Decorator to count number of functions have been called
    # Ideas from
    # https://stackoverflow.com/questions/13852138/how-can-i-define-decorator-method-inside-class
    # https://stackoverflow.com/questions/41678265/how-to-increase-a-number-every-time-a-function-is-run

    @CountAndScope
    @add_arg_scope
    def ConvBNReLUBlock(self, inputs = None, filters = None, kernel_size = None, strides = None, padding = None):
        conv =  self.Conv(inputs, filters, kernel_size, strides, padding)
        bn = self.BN(conv)
        Output = self.ReLU(bn)
        return Output

    @CountAndScope
    @add_arg_scope
    def Conv(self, inputs = None, filters = None, kernel_size = None, strides = None, padding = None, activation=None, name=None):
        Output = tf.layers.conv2d(inputs = inputs, filters = filters, kernel_size = kernel_size,\
                                  strides = strides, padding = padding, activation=activation, name=name) 
        return Output

    @CountAndScope
    @add_arg_scope
    def BN(self, inputs = None):
        Output = tf.layers.batch_normalization(inputs = inputs) 
        return Output
    
    @CountAndScope
    @add_arg_scope
    def ReLU(self, inputs = None):
        Output = tf.nn.relu(inputs)
        return Output

    @CountAndScope
    @add_arg_scope
    def Concat(self, inputs = None, axis=0):
        Output = tf.concat(values = inputs, axis = axis)
        return Output

    @CountAndScope
    @add_arg_scope
    def Flatten(self, inputs = None):
        # https://stackoverflow.com/questions/37868935/tensorflow-reshape-tensor
        Shape = inputs.get_shape().as_list()       
        Dim = numpy.prod(Shape[1:])        
        Output = tf.reshape(inputs, [-1, Dim])         
        return Output

    @CountAndScope
    @add_arg_scope
    def Dropout(self, inputs = None, rate = None):
        if(rate is None):
            rate = 0.5
        Output = tf.layers.dropout(inputs, rate=rate)
        return Output

    @CountAndScope
    @add_arg_scope
    def Dense(self, inputs = None, filters = None, activation=None, name=None):
        Output = tf.layers.dense(inputs, units = filters, activation=activation, name=name)
        return Output

class VanillaNet(BaseLayers):
    def __init__(self, ImageSize = None, InputPH = None, Training = False, WarpType = None, Padding, Opt = None):
        super(SqueezeNet, self).__init__()
        self.ImageSize = ImageSize
        if(InputPH is None):
            print('ERROR: Input PlaceHolder cannot be empty!')
            sys.exit(0)
        if( Opt is None):
            print('ERROR: Options cannot be empty!')
            sys.exit(0)
        self.InputPH = InputPH
        self.InitNeurons = 8
        self.Training = Training
        self.ExpansionFactor = 2.0
        self.DropOutRate = 0.7
        if(padding is None):
            padding = 'same'
        self.Padding = Padding
        self.Opt = Opt

    @CountAndScope
    @add_arg_scope
    def OutputLayer(self, inputs = None, padding = None, rate=None, NumOut=None):
        if(rate is None):
            rate = 0.5
        if(NumOut is None):
           NumOut = self.NumOut     
        flat = self.Flatten(inputs = inputs)
        drop = self.Dropout(inputs = flat, rate=rate)
        dense = self.Dense(inputs = drop, filters = NumOut, activation=None)
        return dense

    @CountAndScope
    @add_arg_scope
    def ICSTNBlock(self, inputs = None, filters = None, NumOut = None):
        # Conv
        Net = self.ConvBNReLUBlock(inputs = inputs, padding = self.Padding, filters = filters, kernel_size = (7,7))
        # Conv
        NumFilters = int(filters*self.ExpansionFactor)
        Net = self.ConvBNReLUBlock(inputs = Net, padding = self.Padding, filters = NumFilters, kernel_size = (5,5))
        # Conv
        NumFilters = int(NumFilters*self.ExpansionFactor)
        Net = self.ConvBNReLUBlock(inputs = Net, padding = self.Padding, filters = NumFilters, kernel_size = (3,3))
        # Conv
        NumFilters = int(NumFilters*self.ExpansionFactor)
        Net = self.ConvBNReLUBlock(inputs = Net, padding = self.Padding, filters = NumFilters, kernel_size = (3,3))
        # Output
        Net = self.OutputLayer(self, inputs = Net, padding = self.Padding, rate=self.DropOutRate, NumOut = NumOut)
        return Net
        
    def _arg_scope(self):
        with arg_scope([self.ConvBNReLUBlock, self.Conv, self.BN, self.ReLU, self.FireModule], kernel_size = (3,3), strides = (2,2), padding = self.Padding) as sc: 
            return sc
        
    def Network(self):
        with arg_scope(self._arg_scope()):
             for count in range(self.Opt.NumBlocks):
                 if(count == 0):
                     pNow = self.Opt.pInit
                     pMtrxNow = warp2.vec2mtrx(self.Opt, pNow)
                # Warp Original Image based on previous composite warp parameters
                ImgWarpNow = warp2.transformImage(self.Opt, self.InputPH, pMtrxNow)

                # Compute current warp parameters
                dpNow = self.ICSTNBlock(self.InputPH,  filters = self.InitNeurons, NumOut = self.Opt.warpDim[count]) 
                dpMtrxNow = warp2.vec2mtrx(self.Opt, dpNow)
                pMtrxNow = warp2.compose(self.Opt, pMtrxNow, dpMtrxNow)

                # Update counter used for looping over warpType
                self.Opt.currBlock += 1
                
            # Decrement counter so you use last warp Type
            self.Opt.currBlock -= 1
            ImgWarp = warp2.transformImage(self.Opt, self.InputPH, pMtrxNow) # Final Image Warp
            pNow = warp2.mtrx2vec(self.opt, pMtrxNow)
            
        return pMtrxNow, pNow, ImgWarp

def main():
   tu.SetGPU(1)
   # Test functionality of code
   InputPH = tf.placeholder(tf.float32, shape=(32, 100, 100, 3), name='Input')
   # Create network class variable
   SN = SqueezeNet(InputPH = InputPH, NumOut = 10)
   # Build the atual network
   Network = SN.Network()
   # Setup Saver
   Saver = tf.train.Saver()
   with tf.Session() as sess:
       # Initialize Weights
       sess.run(tf.global_variables_initializer())
       tu.FindNumParams(1)
       tu.CalculateModelSize(1)
       tu.FindNumFlops(sess, 1)
       # Save model every epoch
       SaveName = '/home/nitin/PRGEye/CheckPoints/SpeedTests/TestSqueezeNet/model.ckpt'
       Saver.save(sess, save_path=SaveName)
       print(SaveName + ' Model Saved...') 
       input('q')
       FeedDict = {SN.InputPH: np.random.rand(32,100,100,3)}
       RetVal = sess.run([Network], feed_dict=FeedDict) 
   Writer = tf.summary.FileWriter('/home/nitin/PRGEye/Logs3/', graph=tf.get_default_graph())

if __name__=="__main__":
    main()
